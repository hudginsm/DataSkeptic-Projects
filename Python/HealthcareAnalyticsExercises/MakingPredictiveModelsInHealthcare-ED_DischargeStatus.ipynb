{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd005286",
   "metadata": {},
   "source": [
    "# Making Predictive Models in Healthcare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69c3dce",
   "metadata": {},
   "source": [
    "## Improting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d44cfc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   width column_name  variable_type\n",
      "0      2      VMONTH    CATEGORICAL\n",
      "1      1       VDAYR    CATEGORICAL\n",
      "2      4     ARRTIME  NONPREDICTIVE\n",
      "3      4    WAITTIME     CONTINUOUS\n",
      "4      4         LOV  NONPREDICTIVE\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('mode.chained_assignment',None)\n",
    "\n",
    "HOME_PATH = r\"~/git_repo/DataSkeptic-Projects/DataSets/NHAMCS_Data_Files/\"\n",
    "\n",
    "df_helper = pd.read_csv(\n",
    "    HOME_PATH + 'ED_metadata.csv',\n",
    "    header=0,\n",
    "    dtype={'width': int, 'column_name': str, 'variable_type': str}\n",
    ")\n",
    "\n",
    "print(df_helper.head(n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1535454",
   "metadata": {},
   "outputs": [],
   "source": [
    "width = df_helper['width'].tolist()\n",
    "col_names = df_helper['column_name'].tolist()\n",
    "var_types = df_helper['variable_type'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "323158e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ed = pd.read_fwf(\n",
    "    HOME_PATH + 'ED2013',\n",
    "    widths=width,\n",
    "    header=None,\n",
    "    dtype='str'\n",
    ")\n",
    "\n",
    "df_ed.columns = col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6af43b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  VMONTH VDAYR ARRTIME WAITTIME   LOV  AGE AGER AGEDAYS RESIDNCE SEX ...   \\\n",
      "0     01     3    0647     0033  0058  046    4     -07       01   2 ...    \n",
      "1     01     3    1841     0109  0150  056    4     -07       01   2 ...    \n",
      "2     01     3    1333     0084  0198  037    3     -07       01   2 ...    \n",
      "3     01     3    1401     0159  0276  007    1     -07       01   1 ...    \n",
      "4     01     4    1947     0114  0248  053    4     -07       01   1 ...    \n",
      "\n",
      "  RX12V3C1 RX12V3C2 RX12V3C3 RX12V3C4 SETTYPE  YEAR   CSTRATM   CPSUM   PATWT  \\\n",
      "0      nan      nan      nan      nan       3  2013  20113201  100020  002945   \n",
      "1      nan      nan      nan      nan       3  2013  20113201  100020  002945   \n",
      "2      nan      nan      nan      nan       3  2013  20113201  100020  002945   \n",
      "3      nan      nan      nan      nan       3  2013  20113201  100020  002945   \n",
      "4      nan      nan      nan      nan       3  2013  20113201  100020  002945   \n",
      "\n",
      "  EDWT  \n",
      "0  nan  \n",
      "1  nan  \n",
      "2  nan  \n",
      "3  nan  \n",
      "4  nan  \n",
      "\n",
      "[5 rows x 579 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_ed.head(n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01449182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24777, 579)\n"
     ]
    }
   ],
   "source": [
    "print(df_ed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5481f2bc",
   "metadata": {},
   "source": [
    "## Making the response variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfde2866",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_cols = ['ADMITHOS','TRANOTH','TRANPSYC','OBSHOS','OBSDIS']\n",
    "\n",
    "df_ed.loc[:, response_cols] = df_ed.loc[:, response_cols].apply(pd.to_numeric)\n",
    "\n",
    "df_ed['ADMITTEMP'] = df_ed[response_cols].sum(axis=1)\n",
    "df_ed['ADMITFINAL'] = 0\n",
    "df_ed.loc[df_ed['ADMITTEMP'] >= 1, 'ADMITFINAL'] = 1\n",
    "\n",
    "df_ed.drop(response_cols, axis=1, inplace=True)\n",
    "df_ed.drop('ADMITTEMP', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5b1624",
   "metadata": {},
   "source": [
    "## Splitting the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a62afcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_target(data, target_name):\n",
    "    target = data[[target_name]]\n",
    "    data.drop(target_name, axis=1, inplace=True)\n",
    "    return (data, target)\n",
    "\n",
    "X, y = split_target(df_ed, 'ADMITFINAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37872ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=1234\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91d6c91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADMITFINAL\n",
      "0    15996\n",
      "1     2586\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_train.groupby('ADMITFINAL').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb15161",
   "metadata": {},
   "source": [
    "## Preprocessing the predictor variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f9ad00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VMONTH\n",
      "01    1757\n",
      "02    1396\n",
      "03    1409\n",
      "04    1719\n",
      "05    2032\n",
      "06    1749\n",
      "07    1696\n",
      "08    1034\n",
      "09    1240\n",
      "10    1306\n",
      "11    1693\n",
      "12    1551\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X_train.groupby('VMONTH').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36902d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_winter(vmonth):\n",
    "    if vmonth in ['12','01','02','03']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "X_train.loc[:,'WINTER'] = df_ed.loc[:,'VMONTH'].apply(is_winter)\n",
    "X_test.loc[:,'WINTER'] = df_ed.loc[:,'VMONTH'].apply(is_winter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b1eba1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WINTER\n",
       "0    12469\n",
       "1     6113\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.groupby('WINTER').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "deb42792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VDAYR\n",
       "1    2559\n",
       "2    2972\n",
       "3    2791\n",
       "4    2632\n",
       "5    2553\n",
       "6    2569\n",
       "7    2506\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.groupby('VDAYR').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d49058ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_night(arrtime):\n",
    "    arrtime_int = int(arrtime)\n",
    "    if ((arrtime_int >= 0) & (arrtime_int < 800)):\n",
    "        return 1\n",
    "    elif ((arrtime_int >= 2000) & (arrtime_int < 2400)):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "X_train.loc[:,'NIGHT'] = df_ed.loc[:,'ARRTIME'].apply(is_night)\n",
    "X_test.loc[:,'NIGHT'] = df_ed.loc[:,'ARRTIME'].apply(is_night)\n",
    "\n",
    "X_train.drop('ARRTIME', axis=1, inplace=True)\n",
    "X_test.drop('ARRTIME', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d6bf88d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NIGHT\n",
       "0    12750\n",
       "1     5832\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.groupby('NIGHT').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "990d9d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.loc[:,'WAITTIME'] = X_train.loc[:,'WAITTIME'].apply(pd.to_numeric)\n",
    "X_test.loc[:,'WAITTIME'] = X_test.loc[:,'WAITTIME'].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88b5693e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_impute_values(data,col):\n",
    "    temp_mean = data.loc[(data[col] != -7) & (data[col] != -9), col].mean()\n",
    "    data.loc[(data[col] == -7) | (data[col] == -9), col] = temp_mean\n",
    "    return data\n",
    "\n",
    "X_train = mean_impute_values(X_train, 'WAITTIME')\n",
    "X_test = mean_impute_values(X_test, 'WAITTIME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37705671",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop('LOV', axis=1, inplace=True)\n",
    "X_test.drop('LOV', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5240e765",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.loc[:,'AGE'] = X_train.loc[:,'AGE'].apply(pd.to_numeric)\n",
    "X_test.loc[:,'AGE'] = X_test.loc[:,'AGE'].apply(pd.to_numeric)\n",
    "\n",
    "X_train.drop('AGEDAYS', axis=1, inplace=True)\n",
    "X_test.drop('AGEDAYS', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c8290b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(['ETHIM','RACER','RACERETH'], axis=1, inplace=True)\n",
    "X_test.drop(['ETHIM','RACER','RACERETH'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8726f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VMONTH</th>\n",
       "      <th>VDAYR</th>\n",
       "      <th>WAITTIME</th>\n",
       "      <th>AGE</th>\n",
       "      <th>AGER</th>\n",
       "      <th>RESIDNCE</th>\n",
       "      <th>SEX</th>\n",
       "      <th>ETHUN</th>\n",
       "      <th>RACEUN</th>\n",
       "      <th>ARREMS</th>\n",
       "      <th>...</th>\n",
       "      <th>RX12V3C3</th>\n",
       "      <th>RX12V3C4</th>\n",
       "      <th>SETTYPE</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>CSTRATM</th>\n",
       "      <th>CPSUM</th>\n",
       "      <th>PATWT</th>\n",
       "      <th>EDWT</th>\n",
       "      <th>WINTER</th>\n",
       "      <th>NIGHT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15938</th>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>58</td>\n",
       "      <td>4</td>\n",
       "      <td>01</td>\n",
       "      <td>1</td>\n",
       "      <td>02</td>\n",
       "      <td>01</td>\n",
       "      <td>01</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>3</td>\n",
       "      <td>2013</td>\n",
       "      <td>40300000</td>\n",
       "      <td>000024</td>\n",
       "      <td>003201</td>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5905</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>91</td>\n",
       "      <td>6</td>\n",
       "      <td>02</td>\n",
       "      <td>1</td>\n",
       "      <td>02</td>\n",
       "      <td>02</td>\n",
       "      <td>01</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>3</td>\n",
       "      <td>2013</td>\n",
       "      <td>20213201</td>\n",
       "      <td>100091</td>\n",
       "      <td>003784</td>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4636</th>\n",
       "      <td>07</td>\n",
       "      <td>1</td>\n",
       "      <td>45.561676</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>01</td>\n",
       "      <td>1</td>\n",
       "      <td>02</td>\n",
       "      <td>02</td>\n",
       "      <td>02</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>3</td>\n",
       "      <td>2013</td>\n",
       "      <td>20213201</td>\n",
       "      <td>100075</td>\n",
       "      <td>002214</td>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9452</th>\n",
       "      <td>08</td>\n",
       "      <td>1</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>01</td>\n",
       "      <td>2</td>\n",
       "      <td>02</td>\n",
       "      <td>-9</td>\n",
       "      <td>02</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>3</td>\n",
       "      <td>2013</td>\n",
       "      <td>20413201</td>\n",
       "      <td>100227</td>\n",
       "      <td>002262</td>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7558</th>\n",
       "      <td>02</td>\n",
       "      <td>4</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>51</td>\n",
       "      <td>4</td>\n",
       "      <td>03</td>\n",
       "      <td>1</td>\n",
       "      <td>02</td>\n",
       "      <td>01</td>\n",
       "      <td>01</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>3</td>\n",
       "      <td>2013</td>\n",
       "      <td>20413201</td>\n",
       "      <td>100242</td>\n",
       "      <td>002108</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 570 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      VMONTH VDAYR   WAITTIME  AGE AGER RESIDNCE SEX ETHUN RACEUN ARREMS  \\\n",
       "15938     11     3  27.000000   58    4       01   1    02     01     01   \n",
       "5905      10     3   5.000000   91    6       02   1    02     02     01   \n",
       "4636      07     1  45.561676   29    3       01   1    02     02     02   \n",
       "9452      08     1  23.000000   20    2       01   2    02     -9     02   \n",
       "7558      02     4  32.000000   51    4       03   1    02     01     01   \n",
       "\n",
       "       ...  RX12V3C3 RX12V3C4 SETTYPE  YEAR   CSTRATM   CPSUM   PATWT EDWT  \\\n",
       "15938  ...       nan      nan       3  2013  40300000  000024  003201  nan   \n",
       "5905   ...       nan      nan       3  2013  20213201  100091  003784  nan   \n",
       "4636   ...       nan      nan       3  2013  20213201  100075  002214  nan   \n",
       "9452   ...       nan      nan       3  2013  20413201  100227  002262  nan   \n",
       "7558   ...       nan      nan       3  2013  20413201  100242  002108  nan   \n",
       "\n",
       "      WINTER NIGHT  \n",
       "15938      0     0  \n",
       "5905       0     1  \n",
       "4636       0     0  \n",
       "9452       0     0  \n",
       "7558       1     0  \n",
       "\n",
       "[5 rows x 570 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2641c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop('PAYTYPER', axis=1, inplace=True)\n",
    "X_test.drop('PAYTYPER', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cef39b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.loc[:,'TEMPF'] = X_train.loc[:,'TEMPF'].apply(pd.to_numeric)\n",
    "X_test.loc[:,'TEMPF'] = X_test.loc[:,'TEMPF'].apply(pd.to_numeric)\n",
    "\n",
    "X_train = mean_impute_values(X_train,'TEMPF')\n",
    "X_test = mean_impute_values(X_test,'TEMPF')\n",
    "\n",
    "X_train.loc[:,'TEMPF'] = X_train.loc[:,'TEMPF'].apply(lambda x: float(x)/10)\n",
    "X_test.loc[:,'TEMPF'] = X_test.loc[:,'TEMPF'].apply(lambda x: float(x)/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1baf56ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15938     98.200000\n",
       "5905      98.100000\n",
       "4636      98.200000\n",
       "9452      98.200000\n",
       "7558      99.300000\n",
       "17878     99.000000\n",
       "21071     97.800000\n",
       "20990     98.600000\n",
       "4537      98.200000\n",
       "7025      99.300000\n",
       "2134      97.500000\n",
       "5212      97.400000\n",
       "9213      97.900000\n",
       "2306      97.000000\n",
       "6106      98.600000\n",
       "2727      98.282103\n",
       "4098      99.100000\n",
       "5233      98.800000\n",
       "5107     100.000000\n",
       "18327     98.900000\n",
       "19242     98.282103\n",
       "3868      97.900000\n",
       "12903     98.600000\n",
       "12763     98.700000\n",
       "8858      99.400000\n",
       "8955      97.900000\n",
       "16360     98.282103\n",
       "6857      97.100000\n",
       "6842      97.700000\n",
       "22073     97.900000\n",
       "Name: TEMPF, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['TEMPF'].head(n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4820656",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.loc[:,'PULSE'] = X_train.loc[:,'PULSE'].apply(pd.to_numeric)\n",
    "X_test.loc[:,'PULSE'] = X_test.loc[:,'PULSE'].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1eb56bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_impute_vitals(data,col):\n",
    "    temp_mean = data.loc[(data[col] != 998) & (data[col] != -9),col].mean()\n",
    "    data.loc[(data[col] == 998) | (data[col] == -9),col] = temp_mean\n",
    "    return data\n",
    "\n",
    "X_train = mean_impute_vitals(X_train, 'PULSE')\n",
    "X_test = mean_impute_vitals(X_test, 'PULSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e35cdb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.loc[:,'RESPR'] = X_train.loc[:,'RESPR'].apply(pd.to_numeric)\n",
    "X_test.loc[:,'RESPR'] = X_test.loc[:,'RESPR'].apply(pd.to_numeric)\n",
    "\n",
    "X_train = mean_impute_values(X_train, 'RESPR')\n",
    "X_test = mean_impute_values(X_test, 'RESPR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3ef8973",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.loc[:,'BPSYS'] = X_train.loc[:,'BPSYS'].apply(pd.to_numeric)\n",
    "X_test.loc[:,'BPSYS'] = X_test.loc[:,'BPSYS'].apply(pd.to_numeric)\n",
    "\n",
    "X_train = mean_impute_values(X_train, 'BPSYS')\n",
    "X_test = mean_impute_values(X_test, 'BPSYS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9304dad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.loc[:,'BPDIAS'] = X_train.loc[:,'BPDIAS'].apply(pd.to_numeric)\n",
    "X_test.loc[:,'BPDIAS'] = X_test.loc[:,'BPDIAS'].apply(pd.to_numeric)\n",
    "\n",
    "def mean_impute_bp_diast(data,col):\n",
    "    temp_mean = data.loc[(data[col] != 998) & (data[col] != -9),col].mean()\n",
    "    data.loc[(data[col] == 998),col] = 40\n",
    "    data.loc[(data[col] == -9),col] = temp_mean\n",
    "    return data\n",
    "\n",
    "X_train = mean_impute_values(X_train, 'BPDIAS')\n",
    "X_test = mean_impute_values(X_test, 'BPDIAS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec501b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.loc[:,'POPCT'] = X_train.loc[:,'POPCT'].apply(pd.to_numeric)\n",
    "X_test.loc[:,'POPCT'] = X_test.loc[:,'POPCT'].apply(pd.to_numeric)\n",
    "\n",
    "X_train = mean_impute_values(X_train, 'POPCT')\n",
    "X_test = mean_impute_values(X_test, 'POPCT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab8918cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEMPF</th>\n",
       "      <th>PULSE</th>\n",
       "      <th>RESPR</th>\n",
       "      <th>BPSYS</th>\n",
       "      <th>BPDIAS</th>\n",
       "      <th>POPCT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15938</th>\n",
       "      <td>98.200000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>22.0</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>98.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5905</th>\n",
       "      <td>98.100000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>96.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4636</th>\n",
       "      <td>98.200000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>98.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9452</th>\n",
       "      <td>98.200000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>98.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7558</th>\n",
       "      <td>99.300000</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>131.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>96.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17878</th>\n",
       "      <td>99.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21071</th>\n",
       "      <td>97.800000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>98.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20990</th>\n",
       "      <td>98.600000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>95.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4537</th>\n",
       "      <td>98.200000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7025</th>\n",
       "      <td>99.300000</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>40.0</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2134</th>\n",
       "      <td>97.500000</td>\n",
       "      <td>91.056517</td>\n",
       "      <td>18.0</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>94.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5212</th>\n",
       "      <td>97.400000</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9213</th>\n",
       "      <td>97.900000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2306</th>\n",
       "      <td>97.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>136.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6106</th>\n",
       "      <td>98.600000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>98.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2727</th>\n",
       "      <td>98.282103</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>92.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4098</th>\n",
       "      <td>99.100000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>133.483987</td>\n",
       "      <td>78.127013</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5233</th>\n",
       "      <td>98.800000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>97.311242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5107</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>24.0</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>94.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18327</th>\n",
       "      <td>98.900000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>98.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            TEMPF       PULSE  RESPR       BPSYS     BPDIAS       POPCT\n",
       "15938   98.200000  101.000000   22.0  159.000000  72.000000   98.000000\n",
       "5905    98.100000   70.000000   18.0  167.000000  79.000000   96.000000\n",
       "4636    98.200000   85.000000   20.0  113.000000  70.000000   98.000000\n",
       "9452    98.200000   84.000000   20.0  146.000000  72.000000   98.000000\n",
       "7558    99.300000  116.000000   18.0  131.000000  82.000000   96.000000\n",
       "17878   99.000000   73.000000   16.0  144.000000  91.000000   99.000000\n",
       "21071   97.800000   88.000000   18.0  121.000000  61.000000   98.000000\n",
       "20990   98.600000   67.000000   16.0  112.000000  65.000000   95.000000\n",
       "4537    98.200000   85.000000   20.0  113.000000  72.000000   99.000000\n",
       "7025    99.300000  172.000000   40.0  124.000000  80.000000  100.000000\n",
       "2134    97.500000   91.056517   18.0  146.000000  75.000000   94.000000\n",
       "5212    97.400000  135.000000   18.0  125.000000  71.000000   99.000000\n",
       "9213    97.900000   85.000000   18.0  153.000000  96.000000   99.000000\n",
       "2306    97.000000   67.000000   20.0  136.000000  75.000000   99.000000\n",
       "6106    98.600000   90.000000   18.0  109.000000  70.000000   98.000000\n",
       "2727    98.282103   83.000000   17.0  123.000000  48.000000   92.000000\n",
       "4098    99.100000  147.000000   20.0  133.483987  78.127013  100.000000\n",
       "5233    98.800000   81.000000   16.0  114.000000  78.000000   97.311242\n",
       "5107   100.000000   95.000000   24.0  133.000000  75.000000   94.000000\n",
       "18327   98.900000   84.000000   16.0  130.000000  85.000000   98.000000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[['TEMPF','PULSE','RESPR','BPSYS','BPDIAS','POPCT']].head(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e7eff940",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.loc[:,'PAINSCALE'] = X_train.loc[:,'PAINSCALE'].apply(pd.to_numeric)\n",
    "X_test.loc[:,'PAINSCALE'] = X_test.loc[:,'PAINSCALE'].apply(pd.to_numeric)\n",
    "\n",
    "def mean_impute_pain(data,col):\n",
    "    temp_mean = data.loc[(data[col] != -8) & (data[col] != -9), col].mean()\n",
    "    data.loc[(data[col] == -8) | (data[col] == -9), col] = temp_mean\n",
    "    return data\n",
    "\n",
    "X_train = mean_impute_pain(X_train, 'PAINSCALE')\n",
    "X_test = mean_impute_pain(X_test, 'PAINSCALE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "90c7fd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfv_codes_path = r\"/home/dataskeptic/git_repo/DataSkeptic-Projects/DataSets/NHAMCS_Data_Files/RFV_CODES.csv\"\n",
    "rfv_codes = pd.read_csv(rfv_codes_path, header=0, dtype='str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ce9c4a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VMONTH</th>\n",
       "      <th>VDAYR</th>\n",
       "      <th>WAITTIME</th>\n",
       "      <th>AGE</th>\n",
       "      <th>AGER</th>\n",
       "      <th>RESIDNCE</th>\n",
       "      <th>SEX</th>\n",
       "      <th>ETHUN</th>\n",
       "      <th>RACEUN</th>\n",
       "      <th>ARREMS</th>\n",
       "      <th>...</th>\n",
       "      <th>rfv_Entry_of_none_or_no_complaint</th>\n",
       "      <th>rfv_Insufficient_information</th>\n",
       "      <th>rfv_Driver's_license_examination_DOT_</th>\n",
       "      <th>rfv_Illegible_entry</th>\n",
       "      <th>rfv_Insurance_examination_</th>\n",
       "      <th>rfv_Disability_examination_</th>\n",
       "      <th>rfv_Worker’s_comp_exam</th>\n",
       "      <th>rfv_Premarital_examination</th>\n",
       "      <th>rfv_Premarital_blood_test</th>\n",
       "      <th>rfv_Direct_admission_to_hospital</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15938</th>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>58</td>\n",
       "      <td>4</td>\n",
       "      <td>01</td>\n",
       "      <td>1</td>\n",
       "      <td>02</td>\n",
       "      <td>01</td>\n",
       "      <td>01</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5905</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>91</td>\n",
       "      <td>6</td>\n",
       "      <td>02</td>\n",
       "      <td>1</td>\n",
       "      <td>02</td>\n",
       "      <td>02</td>\n",
       "      <td>01</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4636</th>\n",
       "      <td>07</td>\n",
       "      <td>1</td>\n",
       "      <td>45.561676</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>01</td>\n",
       "      <td>1</td>\n",
       "      <td>02</td>\n",
       "      <td>02</td>\n",
       "      <td>02</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9452</th>\n",
       "      <td>08</td>\n",
       "      <td>1</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>01</td>\n",
       "      <td>2</td>\n",
       "      <td>02</td>\n",
       "      <td>-9</td>\n",
       "      <td>02</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7558</th>\n",
       "      <td>02</td>\n",
       "      <td>4</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>51</td>\n",
       "      <td>4</td>\n",
       "      <td>03</td>\n",
       "      <td>1</td>\n",
       "      <td>02</td>\n",
       "      <td>01</td>\n",
       "      <td>01</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1264 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      VMONTH VDAYR   WAITTIME  AGE AGER RESIDNCE SEX ETHUN RACEUN ARREMS  \\\n",
       "15938     11     3  27.000000   58    4       01   1    02     01     01   \n",
       "5905      10     3   5.000000   91    6       02   1    02     02     01   \n",
       "4636      07     1  45.561676   29    3       01   1    02     02     02   \n",
       "9452      08     1  23.000000   20    2       01   2    02     -9     02   \n",
       "7558      02     4  32.000000   51    4       03   1    02     01     01   \n",
       "\n",
       "                     ...                rfv_Entry_of_none_or_no_complaint  \\\n",
       "15938                ...                                                0   \n",
       "5905                 ...                                                0   \n",
       "4636                 ...                                                0   \n",
       "9452                 ...                                                0   \n",
       "7558                 ...                                                0   \n",
       "\n",
       "      rfv_Insufficient_information rfv_Driver's_license_examination_DOT_  \\\n",
       "15938                            0                                     0   \n",
       "5905                             0                                     0   \n",
       "4636                             0                                     0   \n",
       "9452                             0                                     0   \n",
       "7558                             0                                     0   \n",
       "\n",
       "      rfv_Illegible_entry rfv_Insurance_examination_  \\\n",
       "15938                   0                          0   \n",
       "5905                    0                          0   \n",
       "4636                    0                          0   \n",
       "9452                    0                          0   \n",
       "7558                    0                          0   \n",
       "\n",
       "      rfv_Disability_examination_ rfv_Worker’s_comp_exam  \\\n",
       "15938                           0                      0   \n",
       "5905                            0                      0   \n",
       "4636                            0                      0   \n",
       "9452                            0                      0   \n",
       "7558                            0                      0   \n",
       "\n",
       "      rfv_Premarital_examination rfv_Premarital_blood_test  \\\n",
       "15938                          0                         0   \n",
       "5905                           0                         0   \n",
       "4636                           0                         0   \n",
       "9452                           0                         0   \n",
       "7558                           0                         0   \n",
       "\n",
       "       rfv_Direct_admission_to_hospital  \n",
       "15938                                 0  \n",
       "5905                                  0  \n",
       "4636                                  0  \n",
       "9452                                  0  \n",
       "7558                                  0  \n",
       "\n",
       "[5 rows x 1264 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from re import sub\n",
    "\n",
    "def add_rfv_column(data,code,desc,rfv_columns):\n",
    "    column_name = \"rfv_\" + sub(' ', '_',desc)\n",
    "    data[column_name] = (data[rfv_columns] == rfv_code).any(axis=1).astype('int')\n",
    "    return data\n",
    "\n",
    "rfv_columns = ['RFV1','RFV2','RFV3']\n",
    "for (rfv_code,rfv_desc) in zip(rfv_codes['Code'].tolist(),rfv_codes['Description'].tolist()):\n",
    "    X_train = add_rfv_column(\n",
    "        X_train,\n",
    "        rfv_code,\n",
    "        rfv_desc,\n",
    "        rfv_columns\n",
    "    )\n",
    "    X_test = add_rfv_column(\n",
    "        X_test,\n",
    "        rfv_code,\n",
    "        rfv_desc,\n",
    "        rfv_columns\n",
    "    )\n",
    "\n",
    "# Remove original RFV columns\n",
    "X_train.drop(rfv_columns, axis=1, inplace=True)\n",
    "X_test.drop(rfv_columns, axis=1, inplace=True)\n",
    "\n",
    "X_train.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4b431aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "inj_cols = [\n",
    "    'INJURY','INJR1','INJR2','INJPOISAD',\n",
    "    'INJPOISADR1','INJPOISADR2','INTENT','INJDETR',\n",
    "    'INJDETR1','INJDETR2','CAUSE1','CAUSE2',\n",
    "    'CAUSE3','CAUSE1R','CAUSE2R','CAUSE3R'\n",
    "]\n",
    "\n",
    "X_train.drop(inj_cols, axis=1, inplace=True)\n",
    "X_test.drop(inj_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f7dc34b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_cols=[\n",
    "    'DIAG1','DIAG2','DIAG3',\n",
    "    'PRDIAG1','PRDIAG2','PRDIAG3',\n",
    "    'DIAG1R','DIAG2R','DIAG3R',\n",
    "]\n",
    "\n",
    "X_train.drop(diag_cols, axis=1, inplace=True)\n",
    "X_test.drop(diag_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "779b86b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.loc[:,'TOTCHRON'] = X_train.loc[:,'TOTCHRON'].apply(pd.to_numeric)\n",
    "X_test.loc[:,'TOTCHRON'] = X_test.loc[:,'TOTCHRON'].apply(pd.to_numeric)\n",
    "\n",
    "X_train = mean_impute_values(X_train,'TOTCHRON')\n",
    "X_test = mean_impute_values(X_test,'TOTCHRON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7625b4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_cols = [\n",
    "    'ABG','BAC','BLOODCX','BNP','BUNCREAT',\n",
    "    'CARDENZ','CBC','DDIMER','ELECTROL','GLUCOSE',\n",
    "    'LACTATE','LFT','PTTINR','OTHERBLD','CARDMON',\n",
    "    'EKG','HIVTEST','FLUTEST','PREGTEST','TOXSCREN',\n",
    "    'URINE','WOUNDCX','URINECX','OTHRTEST','ANYIMAGE',\n",
    "    'XRAY','IVCONTRAST','CATSCAN','CTAB','CTCHEST',\n",
    "    'CTHEAD','CTOTHER','CTUNK','MRI','ULTRASND',\n",
    "    'OTHIMAGE','TOTDIAG','DIAGSCRN'\n",
    "]\n",
    "\n",
    "X_train.drop(testing_cols, axis=1, inplace=True)\n",
    "X_test.drop(testing_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3fabcced",
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_cols = [\n",
    "    'PROC','BPAP','BLADCATH','CASTSPLINT','CENTLINE',\n",
    "    'CPR','ENDOINT','INCDRAIN','IVFLUIDS','LUMBAR',\n",
    "    'NEBUTHER','PELVIC','SKINADH','SUTURE','OTHPROC',\n",
    "    'TOTPROC'\n",
    "]\n",
    "\n",
    "X_train.drop(proc_cols, axis=1, inplace=True)\n",
    "X_test.drop(proc_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f904e9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "med_cols = [\n",
    "    'MED1','MED2','MED3','MED4','MED5',\n",
    "    'MED6','MED7','MED8','MED9','MED10',\n",
    "    'MED11','MED12','GPMED1','GPMED2','GPMED3',\n",
    "    'GPMED4','GPMED5','GPMED6','GPMED7','GPMED8',\n",
    "    'GPMED9','GPMED10','GPMED11','GPMED12','NUMGIV',\n",
    "    'NUMDIS','NUMMED',\n",
    "]\n",
    "\n",
    "X_train.drop(med_cols, axis=1, inplace=True)\n",
    "X_test.drop(med_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0818d6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "prov_cols = [\n",
    "    'NOPROVID','ATTPHYS','RESINT','CONSULT','RNLPN',\n",
    "    'NURSEPR','PHYSASST','EMT','MHPROV','OTHPROV'\n",
    "]\n",
    "\n",
    "X_train.drop(prov_cols, axis=1, inplace=True)\n",
    "X_test.drop(prov_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "72e3f856",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_cols = [\n",
    "    'NODISP','NOFU','RETRNED','RETREFFU','LEFTBTRI',\n",
    "    'LEFTAMA','DOA','DIEDED','TRANNH','OTHDISP',\n",
    "    'ADMIT','ADMTPHYS','BOARDED','LOS','HDDIAG1',\n",
    "    'HDDIAG2','HDDIAG3','HDDIAG1R','HDDIAG2R','HDDIAG3R',\n",
    "    'HDSTAT','ADISP','OBSSTAY','STAY24'\n",
    "]\n",
    "\n",
    "X_train.drop(disp_cols, axis=1, inplace=True)\n",
    "X_test.drop(disp_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3e3a0ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_cols = [\n",
    "    'AGEFL','BDATEFL','SEXFL','ETHNICFL','RACERFL'\n",
    "]\n",
    "\n",
    "X_train.drop(imp_cols, axis=1, inplace=True)\n",
    "X_test.drop(imp_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5372bfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_cols = [\n",
    "    'HOSPCODE','PATCODE'\n",
    "]\n",
    "\n",
    "X_train.drop(id_cols, axis=1, inplace=True)\n",
    "X_test.drop(id_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "811985cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "emr_cols = [\n",
    "    'EBILLANYE','EMRED','HHSMUE','EHRINSE','EDEMOGE',\n",
    "    'EDEMOGER','EPROLSTE','EPROLSTER','EVITALE','EVITALER',\n",
    "    'ESMOKEE','ESMOKEER','EPNOTESE','EPNOTESER','EMEDALGE',\n",
    "    'EMEDALGER','ECPOEE','ECPOEER','ESCRIPE','ESCRIPER',\n",
    "    'EWARNE','EWARNER','EREMINDE','EREMINDER','ECTOEE',\n",
    "    'ECTOEER','EORDERE','EORDERER','ERESULTE','ERESULTER',\n",
    "    'EGRAPHE','EGRAPHER','EIMGRESE','EIMGRESER','EPTEDUE',\n",
    "    'EPTEDUER','ECQME','ECQMER','EGENLISTE','EGENLISTER',\n",
    "    'EIMMREGE','EIMMREGER','ESUME','ESUMER','EMSGE',\n",
    "    'EMSGER','EHLTHINFOE','EHLTHINFOER','EPTRECE','EPTRECER',\n",
    "    'EMEDIDE','EMEDIDER','ESHAREE','ESHAREEHRE','ESHAREWEBE',\n",
    "    'ESHAREOTHE','ESHAREUNKE','ESHAREREFE','LABRESE1','LABRESE2',\n",
    "    'LABRESE3','LABRESE4','LABRESUNKE','LABRESREFE','IMAGREPE1',\n",
    "    'IMAGREPE2','IMAGREPE3','IMAGREPE4','IMAGREPUNKE','IMAGREPREFE',\n",
    "    'PTPROBE1','PTPROBE2','PTPROBE3','PTPROBE4','PTPROBUNKE',\n",
    "    'PTPROBREFE','MEDLISTE1','MEDLISTE2','MEDLISTE3','MEDLISTE4',\n",
    "    'MEDLISTUNKE','MEDLISTREFE','ALGLISTE1','ALGLISTE2','ALGLISTE3',\n",
    "    'ALGLISTE4','ALGLISTUNKE','ALGLISTREFE','EDPRIM','EDINFO',\n",
    "    'MUINC','MUYEAR'\n",
    "]\n",
    "\n",
    "X_train.drop(emr_cols, axis=1, inplace=True)\n",
    "X_test.drop(emr_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1819b424",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_id_cols = [\n",
    "    'DRUGID1','DRUGID2','DRUGID3','DRUGID4','DRUGID5',\n",
    "    'DRUGID6','DRUGID7','DRUGID8','DRUGID9','DRUGID10',\n",
    "    'DRUGID11','DRUGID12'\n",
    "]\n",
    "\n",
    "drug_lev1_cols = [\n",
    "    'RX1V1C1','RX1V1C2','RX1V1C3','RX1V1C4',\n",
    "    'RX2V1C1','RX2V1C2','RX2V1C3','RX2V1C4',\n",
    "    'RX3V1C1','RX3V1C2','RX3V1C3','RX3V1C4',\n",
    "    'RX4V1C1','RX4V1C2','RX4V1C3','RX4V1C4',\n",
    "    'RX5V1C1','RX5V1C2','RX5V1C3','RX5V1C4',\n",
    "    'RX6V1C1','RX6V1C2','RX6V1C3','RX6V1C4',\n",
    "    'RX7V1C1','RX7V1C2','RX7V1C3','RX7V1C4',\n",
    "    'RX8V1C1','RX8V1C2','RX8V1C3','RX8V1C4',\n",
    "    'RX9V1C1','RX9V1C2','RX9V1C3','RX9V1C4',\n",
    "    'RX10V1C1','RX10V1C2','RX10V1C3','RX10V1C4',\n",
    "    'RX11V1C1','RX11V1C2','RX11V1C3','RX11V1C4',\n",
    "    'RX12V1C1','RX12V1C2','RX12V1C3','RX12V1C4'\n",
    "]\n",
    "\n",
    "drug_lev2_cols = [\n",
    "    'RX1V2C1','RX1V2C2','RX1V2C3','RX1V2C4',\n",
    "    'RX2V2C1','RX2V2C2','RX2V2C3','RX2V2C4',\n",
    "    'RX3V2C1','RX3V2C2','RX3V2C3','RX3V2C4',\n",
    "    'RX4V2C1','RX4V2C2','RX4V2C3','RX4V2C4',\n",
    "    'RX5V2C1','RX5V2C2','RX5V2C3','RX5V2C4',\n",
    "    'RX6V2C1','RX6V2C2','RX6V2C3','RX6V2C4',\n",
    "    'RX7V2C1','RX7V2C2','RX7V2C3','RX7V2C4',\n",
    "    'RX8V2C1','RX8V2C2','RX8V2C3','RX8V2C4',\n",
    "    'RX9V2C1','RX9V2C2','RX9V2C3','RX9V2C4',\n",
    "    'RX10V2C1','RX10V2C2','RX10V2C3','RX10V2C4',\n",
    "    'RX11V2C1','RX11V2C2','RX11V2C3','RX11V2C4',\n",
    "    'RX12V2C1','RX12V2C2','RX12V2C3','RX12V2C4'\n",
    "]\n",
    "\n",
    "drug_lev3_cols = [\n",
    "    'RX1V3C1','RX1V3C2','RX1V3C3','RX1V3C4',\n",
    "    'RX2V3C1','RX2V3C2','RX2V3C3','RX2V3C4',\n",
    "    'RX3V3C1','RX3V3C2','RX3V3C3','RX3V3C4',\n",
    "    'RX4V3C1','RX4V3C2','RX4V3C3','RX4V3C4',\n",
    "    'RX5V3C1','RX5V3C2','RX5V3C3','RX5V3C4',\n",
    "    'RX6V3C1','RX6V3C2','RX6V3C3','RX6V3C4',\n",
    "    'RX7V3C1','RX7V3C2','RX7V3C3','RX7V3C4',\n",
    "    'RX8V3C1','RX8V3C2','RX8V3C3','RX8V3C4',\n",
    "    'RX9V3C1','RX9V3C2','RX9V3C3','RX9V3C4',\n",
    "    'RX10V3C1','RX10V3C2','RX10V3C3','RX10V3C4',\n",
    "    'RX11V3C1','RX11V3C2','RX11V3C3','RX11V3C4',\n",
    "    'RX12V3C1','RX12V3C2','RX12V3C3','RX12V3C4'\n",
    "]\n",
    "\n",
    "addl_drug_cols = [\n",
    "    'PRESCR1','CONTSUB1','COMSTAT1','RX1CAT1','RX1CAT2',\n",
    "    'RX1CAT3','RX1CAT4','PRESCR2','CONTSUB2','COMSTAT2',\n",
    "    'RX2CAT1','RX2CAT2','RX2CAT3','RX2CAT4','PRESCR3','CONTSUB3',\n",
    "    'COMSTAT3','RX3CAT1','RX3CAT2','RX3CAT3','RX3CAT4','PRESCR4',\n",
    "    'CONTSUB4','COMSTAT4','RX4CAT1','RX4CAT2','RX4CAT3',\n",
    "    'RX4CAT4','PRESCR5','CONTSUB5','COMSTAT5','RX5CAT1',\n",
    "    'RX5CAT2','RX5CAT3','RX5CAT4','PRESCR6','CONTSUB6',\n",
    "    'COMSTAT6','RX6CAT1','RX6CAT2','RX6CAT3','RX6CAT4','PRESCR7',\n",
    "    'CONTSUB7','COMSTAT7','RX7CAT1','RX7CAT2','RX7CAT3',\n",
    "    'RX7CAT4','PRESCR8','CONTSUB8','COMSTAT8','RX8CAT1',\n",
    "    'RX8CAT2','RX8CAT3','RX8CAT4','PRESCR9','CONTSUB9',\n",
    "    'COMSTAT9','RX9CAT1','RX9CAT2','RX9CAT3','RX9CAT4',\n",
    "    'PRESCR10','CONTSUB10','COMSTAT10','RX10CAT1','RX10CAT2',\n",
    "    'RX10CAT3','RX10CAT4','PRESCR11','CONTSUB11','COMSTAT11',\n",
    "    'RX11CAT1','RX11CAT2','RX11CAT3','RX11CAT4','PRESCR12',\n",
    "    'CONTSUB12','COMSTAT12','RX12CAT1','RX12CAT2','RX12CAT3',\n",
    "    'RX12CAT4'\n",
    "]\n",
    "\n",
    "X_train.drop(drug_id_cols, axis=1, inplace=True)\n",
    "X_train.drop(drug_lev1_cols, axis=1, inplace=True)\n",
    "X_train.drop(drug_lev2_cols, axis=1, inplace=True)\n",
    "X_train.drop(drug_lev3_cols, axis=1, inplace=True)\n",
    "X_train.drop(addl_drug_cols, axis=1, inplace=True)\n",
    "\n",
    "X_test.drop(drug_id_cols, axis=1, inplace=True)\n",
    "X_test.drop(drug_lev1_cols, axis=1, inplace=True)\n",
    "X_test.drop(drug_lev2_cols, axis=1, inplace=True)\n",
    "X_test.drop(drug_lev3_cols, axis=1, inplace=True)\n",
    "X_test.drop(addl_drug_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9051c534",
   "metadata": {},
   "outputs": [],
   "source": [
    "design_cols = ['CSTRATM','CPSUM','PATWT','EDWT']\n",
    "\n",
    "X_train.drop(design_cols, axis=1, inplace=True)\n",
    "X_test.drop(design_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f4aa1d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "categ_cols = df_helper.loc[\n",
    "    df_helper['variable_type'] == 'CATEGORICAL', 'column_name'\n",
    "]\n",
    "one_hot_cols = list(set(categ_cols) & set(X_train.columns))\n",
    "\n",
    "X_train = pd.get_dummies(X_train, columns=one_hot_cols)\n",
    "\n",
    "X_test = pd.get_dummies(X_test, columns=one_hot_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "67610bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.loc[:,X_train.columns] = X_train.loc[:,X_train.columns].apply(pd.to_numeric)\n",
    "X_test.loc[:,X_test.columns] = X_test.loc[:,X_test.columns].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aebd4fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cols = X_train.columns\n",
    "X_test_cols = X_test.columns\n",
    "\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e115be8",
   "metadata": {},
   "source": [
    "## Building the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2230a41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dataskeptic/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.linear_model._logistic.LogisticRegression'>\n",
      "Training accuracy: 0.863200947153159\n",
      "Validation accuracy: 0.8684422921711057\n",
      "                                                column      coef\n",
      "29                                            TOTCHRON  0.064214\n",
      "799                                          ARREMS_01  0.044415\n",
      "1                                                  AGE  0.038154\n",
      "824                                          IMMEDR_02  0.033150\n",
      "840                                          NOCHRON_0  0.032122\n",
      "14                                               RESPR  0.027405\n",
      "838                                          REGDIV_01  0.018929\n",
      "31                                             SURGDAY  0.018715\n",
      "893                                        ZONENURS_01  0.018165\n",
      "856                                            ONO2_01  0.017830\n",
      "55                             rfv_Chest_pain_soreness  0.017310\n",
      "777                                    PHYSPRACTRIA_01  0.016926\n",
      "902                                          ADMDIV_01  0.016917\n",
      "846                                          OBSSEP_01  0.016427\n",
      "906                                        ADVTRIAG_01  0.015718\n",
      "25                                            DIABETES  0.015300\n",
      "12                                               TEMPF  0.015137\n",
      "879                                          AMBDIV_01  0.014669\n",
      "197                            rfv_Shortness_of_breath  0.013850\n",
      "108  rfv_Other_symptoms_or_problems_relating_to_psy...  0.013302\n",
      "23                                                 CHF  0.012687\n",
      "2                                                  SEX  0.012031\n",
      "825                                          IMMEDR_03  0.011738\n",
      "753                                         BEDCZAR_01  0.010927\n",
      "871                                        POOLNURS_01  0.010546\n",
      "746                                           IMBED_01  0.010101\n",
      "13                                               PULSE  0.009975\n",
      "788                                        CATRIAGE_01  0.009131\n",
      "760                                        FASTTRAK_02  0.009111\n",
      "20                                               CEBVD  0.008978\n",
      "..                                                 ...       ...\n",
      "904                                        ADVTRIAG_-8 -0.006250\n",
      "759                                        FASTTRAK_01 -0.006323\n",
      "843                                          OBSSEP_-7 -0.006532\n",
      "898                                         OBSCLIN_02 -0.006532\n",
      "861                                        BOARDHOS_02 -0.006929\n",
      "676                                   rfv_Accident_NOS -0.007275\n",
      "748                                        EXPSPACE_-8 -0.007298\n",
      "33                                              WINTER -0.007332\n",
      "8                                              PAYSELF -0.007720\n",
      "747                                           IMBED_02 -0.007918\n",
      "907                                        ADVTRIAG_02 -0.008114\n",
      "755                                              MED_0 -0.008147\n",
      "872                                        POOLNURS_02 -0.008169\n",
      "850                                        RESIDNCE_01 -0.008573\n",
      "15                                               BPSYS -0.009495\n",
      "754                                         BEDCZAR_02 -0.009758\n",
      "928                                          INCSHX_02 -0.009805\n",
      "880                                          AMBDIV_02 -0.009874\n",
      "847                                          OBSSEP_02 -0.010119\n",
      "811                                           REGION_3 -0.010344\n",
      "929                                       TOTHRDIVR_-7 -0.013314\n",
      "899                                          ADMDIV_-7 -0.013314\n",
      "835                                          REGDIV_-7 -0.013314\n",
      "894                                        ZONENURS_02 -0.013455\n",
      "778                                    PHYSPRACTRIA_02 -0.014096\n",
      "857                                            ONO2_02 -0.023067\n",
      "841                                          NOCHRON_1 -0.032027\n",
      "826                                          IMMEDR_04 -0.036369\n",
      "800                                          ARREMS_02 -0.045484\n",
      "18                                           PAINSCALE -0.071954\n",
      "\n",
      "[941 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clfs = [LogisticRegression()]\n",
    "\n",
    "for clf in clfs:\n",
    "    clf.fit(X_train, y_train.values.ravel())\n",
    "    print(type(clf))\n",
    "    print('Training accuracy: ' + str(clf.score(X_train, y_train.values)))\n",
    "    print('Validation accuracy: ' + str(clf.score(X_test, y_test.values)))\n",
    "    coefs = {\n",
    "        'column': [X_train_cols[i] for i in range(len(X_train_cols))],\n",
    "        'coef': [clf.coef_[0,i] for i in range(len(X_train_cols))]\n",
    "    }\n",
    "    df_coefs = pd.DataFrame(coefs)\n",
    "    print(df_coefs.sort_values('coef', axis=0, ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a377d9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.ensemble._forest.RandomForestClassifier'>\n",
      "Training accuracy: 0.9999461844796039\n",
      "Validation accuracy: 0.8860371267150928\n",
      "                                                column       imp\n",
      "1                                                  AGE  0.041547\n",
      "13                                               PULSE  0.027663\n",
      "15                                               BPSYS  0.026291\n",
      "16                                              BPDIAS  0.026208\n",
      "0                                             WAITTIME  0.024478\n",
      "12                                               TEMPF  0.024179\n",
      "17                                               POPCT  0.021141\n",
      "14                                               RESPR  0.020849\n",
      "29                                            TOTCHRON  0.017294\n",
      "18                                           PAINSCALE  0.016473\n",
      "800                                          ARREMS_02  0.016127\n",
      "824                                          IMMEDR_02  0.015723\n",
      "799                                          ARREMS_01  0.015194\n",
      "841                                          NOCHRON_1  0.010663\n",
      "840                                          NOCHRON_0  0.010305\n",
      "940                                             AGER_6  0.009773\n",
      "856                                            ONO2_01  0.008935\n",
      "108  rfv_Other_symptoms_or_problems_relating_to_psy...  0.008443\n",
      "5                                             PAYMCARE  0.008347\n",
      "55                             rfv_Chest_pain_soreness  0.008103\n",
      "31                                             SURGDAY  0.007419\n",
      "2                                                  SEX  0.007293\n",
      "826                                          IMMEDR_04  0.007227\n",
      "23                                                 CHF  0.007226\n",
      "197                            rfv_Shortness_of_breath  0.006782\n",
      "34                                               NIGHT  0.006001\n",
      "4                                              PAYPRIV  0.005997\n",
      "825                                          IMMEDR_03  0.005618\n",
      "25                                            DIABETES  0.005543\n",
      "938                                             AGER_4  0.005276\n",
      "..                                                 ...       ...\n",
      "682                                    rfv_Elder_abuse  0.000000\n",
      "681                               rfv_Battered_spouse_  0.000000\n",
      "452                     rfv_Attention_deficit_disorder  0.000000\n",
      "454          rfv_Parkinson's_disease_paralysis_agitans  0.000000\n",
      "459                               rfv_Refractive_error  0.000000\n",
      "722  rfv_Physical_examination_for_extracurricular_a...  0.000000\n",
      "460                                       rfv_Cataract  0.000000\n",
      "461                                       rfv_Glaucoma  0.000000\n",
      "462                      rfv_Other_diseases_of_the_eye  0.000000\n",
      "463                                  rfv_Otitis_media_  0.000000\n",
      "465  rfv_Rheumatic_fever_and_chronic_rheumatic_hear...  0.000000\n",
      "466  rfv_Hypertension_with_involvement_of_target_or...  0.000000\n",
      "686                         rfv_Cardiopulmonary_arrest  0.000000\n",
      "437                  rfv_Diseases_of_the_thyroid_gland  0.000000\n",
      "693                                 rfv_Food_poisoning  0.000000\n",
      "436                   rfv_Neoplasm_of_uncertain_nature  0.000000\n",
      "698                              rfv_Alcohol_poisoning  0.000000\n",
      "700            rfv_Adverse_effects_of_secondhand_smoke  0.000000\n",
      "701  rfv_Adverse_effects_of_terrorism_and_bioterrorism  0.000000\n",
      "706                          rfv_For_cytology_findings  0.000000\n",
      "711  rfv_For_results_of_cholesterol_and_triglycerid...  0.000000\n",
      "418                 rfv_Intestinal_infectious_diseases  0.000000\n",
      "713  rfv_For_results_of_test_for_human_immunodefici...  0.000000\n",
      "714  rfv_Physical_examination_required_for_school_o...  0.000000\n",
      "417                             rfv_Hemorrhagic_fevers  0.000000\n",
      "717                 rfv_Executive_physical_examination  0.000000\n",
      "718       rfv_Physical_examination_required_for_school  0.000000\n",
      "415                        rfv_Pigeontoed_feet_turn_in  0.000000\n",
      "720                rfv_Patient_unable_to_speak_English  0.000000\n",
      "233                                 rfv_Lung_infection  0.000000\n",
      "\n",
      "[941 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clfs_rf = [RandomForestClassifier(n_estimators=100)]\n",
    "\n",
    "for clf in clfs_rf:\n",
    "    clf.fit(X_train, y_train.values.ravel())\n",
    "    print(type(clf))\n",
    "    print('Training accuracy: ' + str(clf.score(X_train, y_train.values)))\n",
    "    print('Validation accuracy: ' + str(clf.score(X_test, y_test.values)))\n",
    "    imps = {\n",
    "        'column': [X_train_cols[i] for i in range(len(X_train_cols))],\n",
    "        'imp': [clf.feature_importances_[i] for i in range(len(X_train_cols))]\n",
    "    }\n",
    "    df_imps = pd.DataFrame(imps)\n",
    "    print(df_imps.sort_values('imp', axis=0, ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "422fbfa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150-unit network:\n",
      "Iteration 1, loss = inf\n",
      "Iteration 2, loss = 0.26256205\n",
      "Iteration 3, loss = 0.22600882\n",
      "Iteration 4, loss = 0.20554737\n",
      "Iteration 5, loss = 0.18788855\n",
      "Iteration 6, loss = 0.17089986\n",
      "Iteration 7, loss = 0.15399132\n",
      "Iteration 8, loss = 0.13804192\n",
      "Iteration 9, loss = 0.12242161\n",
      "Iteration 10, loss = 0.10969702\n",
      "Iteration 11, loss = 0.09644333\n",
      "Iteration 12, loss = 0.08600169\n",
      "Iteration 13, loss = 0.07545349\n",
      "Iteration 14, loss = 0.06576426\n",
      "Iteration 15, loss = 0.05744720\n",
      "Iteration 16, loss = 0.05057766\n",
      "Iteration 17, loss = 0.04424809\n",
      "Iteration 18, loss = 0.03876858\n",
      "Iteration 19, loss = 0.03392434\n",
      "Iteration 20, loss = 0.03008661\n",
      "Iteration 21, loss = 0.02604014\n",
      "Iteration 22, loss = 0.02280625\n",
      "Iteration 23, loss = 0.02095042\n",
      "Iteration 24, loss = 0.01865465\n",
      "Iteration 25, loss = 0.01618499\n",
      "Iteration 26, loss = 0.01559927\n",
      "Iteration 27, loss = 0.01284502\n",
      "Iteration 28, loss = 0.01190597\n",
      "Iteration 29, loss = 0.01417864\n",
      "Iteration 30, loss = 0.01043900\n",
      "Iteration 31, loss = 0.00889180\n",
      "Iteration 32, loss = 0.00853581\n",
      "Iteration 33, loss = 0.00731132\n",
      "Iteration 34, loss = 0.00678754\n",
      "Iteration 35, loss = 0.00726278\n",
      "Iteration 36, loss = 0.00614964\n",
      "Iteration 37, loss = 0.00630200\n",
      "Iteration 38, loss = 0.00452416\n",
      "Iteration 39, loss = 0.00405030\n",
      "Iteration 40, loss = 0.00378192\n",
      "Iteration 41, loss = 0.00332881\n",
      "Iteration 42, loss = 0.00448414\n",
      "Iteration 43, loss = 0.00404074\n",
      "Iteration 44, loss = 0.00336697\n",
      "Iteration 45, loss = 0.00249601\n",
      "Iteration 46, loss = 0.00234698\n",
      "Iteration 47, loss = 0.00205410\n",
      "Iteration 48, loss = 0.00240161\n",
      "Iteration 49, loss = 0.00202548\n",
      "Iteration 50, loss = 0.00169365\n",
      "Iteration 51, loss = 0.00155263\n",
      "Iteration 52, loss = 0.00146814\n",
      "Iteration 53, loss = 0.00138274\n",
      "Iteration 54, loss = 0.00128714\n",
      "Iteration 55, loss = 0.00120946\n",
      "Iteration 56, loss = 0.00113969\n",
      "Iteration 57, loss = 0.00112013\n",
      "Iteration 58, loss = 0.00104426\n",
      "Iteration 59, loss = 0.00098440\n",
      "Iteration 60, loss = 0.00137165\n",
      "Iteration 61, loss = 0.00324280\n",
      "Iteration 62, loss = inf\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training accuracy: 0.9770207727908728\n",
      "Validation accuracy: 0.853591606133979\n",
      "100-unit network:\n",
      "Iteration 1, loss = 0.36880070\n",
      "Iteration 2, loss = 0.25646291\n",
      "Iteration 3, loss = 0.22770107\n",
      "Iteration 4, loss = 0.20664510\n",
      "Iteration 5, loss = 0.18851266\n",
      "Iteration 6, loss = 0.17401206\n",
      "Iteration 7, loss = 0.15686754\n",
      "Iteration 8, loss = 0.14251539\n",
      "Iteration 9, loss = 0.12781202\n",
      "Iteration 10, loss = 0.11442400\n",
      "Iteration 11, loss = 0.10199422\n",
      "Iteration 12, loss = 0.09154763\n",
      "Iteration 13, loss = 0.08114275\n",
      "Iteration 14, loss = 0.07073716\n",
      "Iteration 15, loss = 0.06292958\n",
      "Iteration 16, loss = 0.05669079\n",
      "Iteration 17, loss = 0.05081503\n",
      "Iteration 18, loss = 0.04505124\n",
      "Iteration 19, loss = 0.03931957\n",
      "Iteration 20, loss = 0.03631059\n",
      "Iteration 21, loss = 0.03190060\n",
      "Iteration 22, loss = 0.02899532\n",
      "Iteration 23, loss = 0.02563753\n",
      "Iteration 24, loss = 0.02346188\n",
      "Iteration 25, loss = 0.02055516\n",
      "Iteration 26, loss = 0.01902900\n",
      "Iteration 27, loss = 0.01733808\n",
      "Iteration 28, loss = 0.01498524\n",
      "Iteration 29, loss = 0.01354244\n",
      "Iteration 30, loss = 0.01224127\n",
      "Iteration 31, loss = 0.01310427\n",
      "Iteration 32, loss = 0.01088501\n",
      "Iteration 33, loss = 0.00936778\n",
      "Iteration 34, loss = 0.00857202\n",
      "Iteration 35, loss = 0.00798336\n",
      "Iteration 36, loss = 0.00827848\n",
      "Iteration 37, loss = 0.00715825\n",
      "Iteration 38, loss = 0.00681709\n",
      "Iteration 39, loss = 0.00582742\n",
      "Iteration 40, loss = 0.00557799\n",
      "Iteration 41, loss = 0.00523537\n",
      "Iteration 42, loss = 0.00527320\n",
      "Iteration 43, loss = 0.00458860\n",
      "Iteration 44, loss = 0.00569868\n",
      "Iteration 45, loss = 0.00435671\n",
      "Iteration 46, loss = 0.00373612\n",
      "Iteration 47, loss = 0.00360061\n",
      "Iteration 48, loss = 0.00327233\n",
      "Iteration 49, loss = 0.00321266\n",
      "Iteration 50, loss = 0.00252015\n",
      "Iteration 51, loss = 0.00223201\n",
      "Iteration 52, loss = 0.00227619\n",
      "Iteration 53, loss = 0.00187899\n",
      "Iteration 54, loss = 0.00180870\n",
      "Iteration 55, loss = 0.00173034\n",
      "Iteration 56, loss = 0.00164505\n",
      "Iteration 57, loss = 0.00146064\n",
      "Iteration 58, loss = 0.00135922\n",
      "Iteration 59, loss = 0.00132393\n",
      "Iteration 60, loss = 0.00121781\n",
      "Iteration 61, loss = 0.00116130\n",
      "Iteration 62, loss = 0.00111501\n",
      "Iteration 63, loss = 0.00159247\n",
      "Iteration 64, loss = 0.01915920\n",
      "Iteration 65, loss = inf\n",
      "Iteration 66, loss = inf\n",
      "Iteration 67, loss = 0.03691387\n",
      "Iteration 68, loss = 0.01395291\n",
      "Iteration 69, loss = 0.00549132\n",
      "Iteration 70, loss = 0.00304386\n",
      "Iteration 71, loss = 0.00260285\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training accuracy: 1.0\n",
      "Validation accuracy: 0.8716707021791767\n",
      "80-unit network:\n",
      "Iteration 1, loss = 0.35038057\n",
      "Iteration 2, loss = 0.25851637\n",
      "Iteration 3, loss = 0.23385806\n",
      "Iteration 4, loss = 0.21622603\n",
      "Iteration 5, loss = 0.20020063\n",
      "Iteration 6, loss = 0.18447580\n",
      "Iteration 7, loss = 0.16917738\n",
      "Iteration 8, loss = 0.15459755\n",
      "Iteration 9, loss = 0.13910452\n",
      "Iteration 10, loss = 0.12532082\n",
      "Iteration 11, loss = 0.11205313\n",
      "Iteration 12, loss = 0.10131371\n",
      "Iteration 13, loss = 0.09115770\n",
      "Iteration 14, loss = 0.08208124\n",
      "Iteration 15, loss = 0.07280838\n",
      "Iteration 16, loss = 0.06524247\n",
      "Iteration 17, loss = 0.05794104\n",
      "Iteration 18, loss = 0.05250394\n",
      "Iteration 19, loss = 0.04673973\n",
      "Iteration 20, loss = 0.04207561\n",
      "Iteration 21, loss = 0.03814665\n",
      "Iteration 22, loss = 0.03473714\n",
      "Iteration 23, loss = 0.03136152\n",
      "Iteration 24, loss = 0.02786557\n",
      "Iteration 25, loss = 0.02571095\n",
      "Iteration 26, loss = 0.02400974\n",
      "Iteration 27, loss = 0.02100652\n",
      "Iteration 28, loss = 0.01947397\n",
      "Iteration 29, loss = 0.01829317\n",
      "Iteration 30, loss = 0.01688089\n",
      "Iteration 31, loss = 0.01511864\n",
      "Iteration 32, loss = 0.01330187\n",
      "Iteration 33, loss = 0.01249664\n",
      "Iteration 34, loss = 0.01343146\n",
      "Iteration 35, loss = 0.01119000\n",
      "Iteration 36, loss = 0.01066005\n",
      "Iteration 37, loss = 0.00947560\n",
      "Iteration 38, loss = 0.00839750\n",
      "Iteration 39, loss = 0.00722975\n",
      "Iteration 40, loss = 0.00677634\n",
      "Iteration 41, loss = 0.00614782\n",
      "Iteration 42, loss = 0.00687645\n",
      "Iteration 43, loss = 0.00662387\n",
      "Iteration 44, loss = 0.00524840\n",
      "Iteration 45, loss = 0.00607653\n",
      "Iteration 46, loss = 0.00547672\n",
      "Iteration 47, loss = 0.00612614\n",
      "Iteration 48, loss = 0.00557383\n",
      "Iteration 49, loss = 0.00409764\n",
      "Iteration 50, loss = 0.00352548\n",
      "Iteration 51, loss = 0.00309937\n",
      "Iteration 52, loss = 0.00280661\n",
      "Iteration 53, loss = 0.00248129\n",
      "Iteration 54, loss = 0.00235862\n",
      "Iteration 55, loss = 0.00242542\n",
      "Iteration 56, loss = 0.00238301\n",
      "Iteration 57, loss = 0.00285801\n",
      "Iteration 58, loss = 0.00215480\n",
      "Iteration 59, loss = 0.00180646\n",
      "Iteration 60, loss = 0.00166097\n",
      "Iteration 61, loss = 0.00158000\n",
      "Iteration 62, loss = 0.00208965\n",
      "Iteration 63, loss = 0.00649157\n",
      "Iteration 64, loss = 0.01468194\n",
      "Iteration 65, loss = 0.06242453\n",
      "Iteration 66, loss = inf\n",
      "Iteration 67, loss = inf\n",
      "Iteration 68, loss = 0.01006956\n",
      "Iteration 69, loss = 0.00350564\n",
      "Iteration 70, loss = 0.00251821\n",
      "Iteration 71, loss = 0.00210725\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training accuracy: 1.0\n",
      "Validation accuracy: 0.8692493946731235\n",
      "60-unit network:\n",
      "Iteration 1, loss = 0.43817693\n",
      "Iteration 2, loss = 0.27858455\n",
      "Iteration 3, loss = 0.24425850\n",
      "Iteration 4, loss = 0.22456096\n",
      "Iteration 5, loss = 0.21123150\n",
      "Iteration 6, loss = 0.19883836\n",
      "Iteration 7, loss = 0.18702645\n",
      "Iteration 8, loss = 0.17639495\n",
      "Iteration 9, loss = 0.16424704\n",
      "Iteration 10, loss = 0.15323524\n",
      "Iteration 11, loss = 0.14198278\n",
      "Iteration 12, loss = 0.13180897\n",
      "Iteration 13, loss = 0.12124541\n",
      "Iteration 14, loss = 0.11154086\n",
      "Iteration 15, loss = 0.10328174\n",
      "Iteration 16, loss = 0.09524059\n",
      "Iteration 17, loss = 0.08718674\n",
      "Iteration 18, loss = 0.07983181\n",
      "Iteration 19, loss = 0.07305628\n",
      "Iteration 20, loss = 0.06742542\n",
      "Iteration 21, loss = 0.06120079\n",
      "Iteration 22, loss = 0.05755006\n",
      "Iteration 23, loss = 0.05277260\n",
      "Iteration 24, loss = 0.04750500\n",
      "Iteration 25, loss = 0.04454762\n",
      "Iteration 26, loss = 0.04068632\n",
      "Iteration 27, loss = 0.03760216\n",
      "Iteration 28, loss = 0.03469558\n",
      "Iteration 29, loss = 0.03138806\n",
      "Iteration 30, loss = 0.02904979\n",
      "Iteration 31, loss = 0.02681987\n",
      "Iteration 32, loss = 0.02507545\n",
      "Iteration 33, loss = 0.02332427\n",
      "Iteration 34, loss = 0.02147358\n",
      "Iteration 35, loss = 0.01934402\n",
      "Iteration 36, loss = 0.01762846\n",
      "Iteration 37, loss = 0.01632318\n",
      "Iteration 38, loss = 0.01491593\n",
      "Iteration 39, loss = 0.01396221\n",
      "Iteration 40, loss = 0.01285356\n",
      "Iteration 41, loss = 0.01315166\n",
      "Iteration 42, loss = 0.01219127\n",
      "Iteration 43, loss = 0.01077855\n",
      "Iteration 44, loss = 0.01005511\n",
      "Iteration 45, loss = 0.00965608\n",
      "Iteration 46, loss = 0.00856972\n",
      "Iteration 47, loss = 0.00784812\n",
      "Iteration 48, loss = 0.00754257\n",
      "Iteration 49, loss = 0.00767615\n",
      "Iteration 50, loss = 0.00691687\n",
      "Iteration 51, loss = 0.00715804\n",
      "Iteration 52, loss = 0.00601868\n",
      "Iteration 53, loss = 0.00601794\n",
      "Iteration 54, loss = 0.00506373\n",
      "Iteration 55, loss = 0.00431951\n",
      "Iteration 56, loss = 0.00411957\n",
      "Iteration 57, loss = 0.00386095\n",
      "Iteration 58, loss = 0.00373896\n",
      "Iteration 59, loss = 0.00352112\n",
      "Iteration 60, loss = 0.00452409\n",
      "Iteration 61, loss = 0.00334730\n",
      "Iteration 62, loss = 0.00328661\n",
      "Iteration 63, loss = 0.00312576\n",
      "Iteration 64, loss = 0.00288462\n",
      "Iteration 65, loss = 0.00406642\n",
      "Iteration 66, loss = 0.00255519\n",
      "Iteration 67, loss = 0.00294879\n",
      "Iteration 68, loss = 0.00207899\n",
      "Iteration 69, loss = 0.00195704\n",
      "Iteration 70, loss = 0.00640482\n",
      "Iteration 71, loss = 0.01111804\n",
      "Iteration 72, loss = 0.03672190\n",
      "Iteration 73, loss = inf\n",
      "Iteration 74, loss = inf\n",
      "Iteration 75, loss = 0.01542327\n",
      "Iteration 76, loss = 0.00672650\n",
      "Iteration 77, loss = 0.00271739\n",
      "Iteration 78, loss = 0.00209499\n",
      "Iteration 79, loss = 0.00188292\n",
      "Iteration 80, loss = 0.00173607\n",
      "Iteration 81, loss = 0.00163048\n",
      "Iteration 82, loss = 0.00155361\n",
      "Iteration 83, loss = 0.00148816\n",
      "Iteration 84, loss = 0.00140821\n",
      "Iteration 85, loss = 0.00134676\n",
      "Iteration 86, loss = 0.00130260\n",
      "Iteration 87, loss = 0.00124670\n",
      "Iteration 88, loss = 0.00119921\n",
      "Iteration 89, loss = 0.00115973\n",
      "Iteration 90, loss = 0.00112639\n",
      "Iteration 91, loss = 0.00109500\n",
      "Iteration 92, loss = 0.00104413\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training accuracy: 1.0\n",
      "Validation accuracy: 0.8679580306698951\n",
      "40-unit network:\n",
      "Iteration 1, loss = 0.37403864\n",
      "Iteration 2, loss = 0.27302279\n",
      "Iteration 3, loss = 0.24582073\n",
      "Iteration 4, loss = 0.22968019\n",
      "Iteration 5, loss = 0.21694754\n",
      "Iteration 6, loss = 0.20570715\n",
      "Iteration 7, loss = 0.19470934\n",
      "Iteration 8, loss = 0.18348185\n",
      "Iteration 9, loss = 0.17307595\n",
      "Iteration 10, loss = 0.16280656\n",
      "Iteration 11, loss = 0.15314160\n",
      "Iteration 12, loss = 0.14235415\n",
      "Iteration 13, loss = 0.13362203\n",
      "Iteration 14, loss = 0.12420894\n",
      "Iteration 15, loss = 0.11619966\n",
      "Iteration 16, loss = 0.10797348\n",
      "Iteration 17, loss = 0.10049425\n",
      "Iteration 18, loss = 0.09410508\n",
      "Iteration 19, loss = 0.08754971\n",
      "Iteration 20, loss = 0.08197259\n",
      "Iteration 21, loss = 0.07672054\n",
      "Iteration 22, loss = 0.07206771\n",
      "Iteration 23, loss = 0.06620573\n",
      "Iteration 24, loss = 0.06170212\n",
      "Iteration 25, loss = 0.05787302\n",
      "Iteration 26, loss = 0.05505496\n",
      "Iteration 27, loss = 0.05077651\n",
      "Iteration 28, loss = 0.04737504\n",
      "Iteration 29, loss = 0.04486982\n",
      "Iteration 30, loss = 0.04219677\n",
      "Iteration 31, loss = 0.03880723\n",
      "Iteration 32, loss = 0.03594597\n",
      "Iteration 33, loss = 0.03416139\n",
      "Iteration 34, loss = 0.03205569\n",
      "Iteration 35, loss = 0.03037371\n",
      "Iteration 36, loss = 0.02865080\n",
      "Iteration 37, loss = 0.02660271\n",
      "Iteration 38, loss = 0.02500380\n",
      "Iteration 39, loss = 0.02400850\n",
      "Iteration 40, loss = 0.02176151\n",
      "Iteration 41, loss = 0.02046101\n",
      "Iteration 42, loss = 0.01926808\n",
      "Iteration 43, loss = 0.01780792\n",
      "Iteration 44, loss = 0.01698025\n",
      "Iteration 45, loss = 0.01561185\n",
      "Iteration 46, loss = 0.01494203\n",
      "Iteration 47, loss = 0.01412321\n",
      "Iteration 48, loss = 0.01356049\n",
      "Iteration 49, loss = 0.01242303\n",
      "Iteration 50, loss = 0.01184132\n",
      "Iteration 51, loss = 0.01118300\n",
      "Iteration 52, loss = 0.01045527\n",
      "Iteration 53, loss = 0.01007707\n",
      "Iteration 54, loss = 0.00989266\n",
      "Iteration 55, loss = 0.00989724\n",
      "Iteration 56, loss = 0.00901639\n",
      "Iteration 57, loss = 0.00864351\n",
      "Iteration 58, loss = 0.00749813\n",
      "Iteration 59, loss = 0.00689716\n",
      "Iteration 60, loss = 0.00628944\n",
      "Iteration 61, loss = 0.00659809\n",
      "Iteration 62, loss = 0.00872488\n",
      "Iteration 63, loss = 0.00762248\n",
      "Iteration 64, loss = 0.00639217\n",
      "Iteration 65, loss = 0.00517564\n",
      "Iteration 66, loss = 0.00541398\n",
      "Iteration 67, loss = 0.00428032\n",
      "Iteration 68, loss = 0.00393974\n",
      "Iteration 69, loss = 0.00379401\n",
      "Iteration 70, loss = 0.00364862\n",
      "Iteration 71, loss = 0.00393168\n",
      "Iteration 72, loss = 0.00356560\n",
      "Iteration 73, loss = 0.00329181\n",
      "Iteration 74, loss = 0.00330152\n",
      "Iteration 75, loss = 0.00328149\n",
      "Iteration 76, loss = 0.00337082\n",
      "Iteration 77, loss = 0.00361878\n",
      "Iteration 78, loss = 0.00345392\n",
      "Iteration 79, loss = 0.00578455\n",
      "Iteration 80, loss = 0.00669706\n",
      "Iteration 81, loss = 0.00655965\n",
      "Iteration 82, loss = 0.00911628\n",
      "Iteration 83, loss = 0.01119336\n",
      "Iteration 84, loss = 0.01943549\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training accuracy: 0.9946184479603918\n",
      "Validation accuracy: 0.8592413236481034\n",
      "20-unit network:\n",
      "Iteration 1, loss = 0.64488965\n",
      "Iteration 2, loss = 0.33699477\n",
      "Iteration 3, loss = 0.28394824\n",
      "Iteration 4, loss = 0.25955328\n",
      "Iteration 5, loss = 0.24452949\n",
      "Iteration 6, loss = 0.23314912\n",
      "Iteration 7, loss = 0.22404450\n",
      "Iteration 8, loss = 0.21680497\n",
      "Iteration 9, loss = 0.20995547\n",
      "Iteration 10, loss = 0.20366542\n",
      "Iteration 11, loss = 0.19777391\n",
      "Iteration 12, loss = 0.19222061\n",
      "Iteration 13, loss = 0.18681523\n",
      "Iteration 14, loss = 0.18119378\n",
      "Iteration 15, loss = 0.17594763\n",
      "Iteration 16, loss = 0.17063455\n",
      "Iteration 17, loss = 0.16490899\n",
      "Iteration 18, loss = 0.16002354\n",
      "Iteration 19, loss = 0.15546557\n",
      "Iteration 20, loss = 0.15041737\n",
      "Iteration 21, loss = 0.14560646\n",
      "Iteration 22, loss = 0.14121048\n",
      "Iteration 23, loss = 0.13639318\n",
      "Iteration 24, loss = 0.13216052\n",
      "Iteration 25, loss = 0.12833513\n",
      "Iteration 26, loss = 0.12431364\n",
      "Iteration 27, loss = 0.12053575\n",
      "Iteration 28, loss = 0.11668908\n",
      "Iteration 29, loss = 0.11334639\n",
      "Iteration 30, loss = 0.11048597\n",
      "Iteration 31, loss = 0.10659631\n",
      "Iteration 32, loss = 0.10379056\n",
      "Iteration 33, loss = 0.10111673\n",
      "Iteration 34, loss = 0.09780462\n",
      "Iteration 35, loss = 0.09541480\n",
      "Iteration 36, loss = 0.09325233\n",
      "Iteration 37, loss = 0.09014160\n",
      "Iteration 38, loss = 0.08794913\n",
      "Iteration 39, loss = 0.08597688\n",
      "Iteration 40, loss = 0.08379815\n",
      "Iteration 41, loss = 0.08116316\n",
      "Iteration 42, loss = 0.07927411\n",
      "Iteration 43, loss = 0.07767721\n",
      "Iteration 44, loss = 0.07513554\n",
      "Iteration 45, loss = 0.07327175\n",
      "Iteration 46, loss = 0.07090692\n",
      "Iteration 47, loss = 0.06956468\n",
      "Iteration 48, loss = 0.06766098\n",
      "Iteration 49, loss = 0.06592468\n",
      "Iteration 50, loss = 0.06395941\n",
      "Iteration 51, loss = 0.06300669\n",
      "Iteration 52, loss = 0.06091510\n",
      "Iteration 53, loss = 0.06002662\n",
      "Iteration 54, loss = 0.05833379\n",
      "Iteration 55, loss = 0.05653391\n",
      "Iteration 56, loss = 0.05565230\n",
      "Iteration 57, loss = 0.05426539\n",
      "Iteration 58, loss = 0.05277837\n",
      "Iteration 59, loss = 0.05148421\n",
      "Iteration 60, loss = 0.05060268\n",
      "Iteration 61, loss = 0.04928858\n",
      "Iteration 62, loss = 0.04828553\n",
      "Iteration 63, loss = 0.04734350\n",
      "Iteration 64, loss = 0.04521976\n",
      "Iteration 65, loss = 0.04484503\n",
      "Iteration 66, loss = 0.04384149\n",
      "Iteration 67, loss = 0.04226952\n",
      "Iteration 68, loss = 0.04184007\n",
      "Iteration 69, loss = 0.04082196\n",
      "Iteration 70, loss = 0.03974299\n",
      "Iteration 71, loss = 0.03958260\n",
      "Iteration 72, loss = 0.03823698\n",
      "Iteration 73, loss = 0.03744212\n",
      "Iteration 74, loss = 0.03644276\n",
      "Iteration 75, loss = 0.03640620\n",
      "Iteration 76, loss = 0.03554688\n",
      "Iteration 77, loss = 0.03523425\n",
      "Iteration 78, loss = 0.03322769\n",
      "Iteration 79, loss = 0.03268690\n",
      "Iteration 80, loss = 0.03204452\n",
      "Iteration 81, loss = 0.03139948\n",
      "Iteration 82, loss = 0.03060374\n",
      "Iteration 83, loss = 0.02953289\n",
      "Iteration 84, loss = 0.02993034\n",
      "Iteration 85, loss = 0.02883688\n",
      "Iteration 86, loss = 0.02783414\n",
      "Iteration 87, loss = 0.02744796\n",
      "Iteration 88, loss = 0.02675693\n",
      "Iteration 89, loss = 0.02637432\n",
      "Iteration 90, loss = 0.02595764\n",
      "Iteration 91, loss = 0.02486051\n",
      "Iteration 92, loss = 0.02468510\n",
      "Iteration 93, loss = 0.02449379\n",
      "Iteration 94, loss = 0.02451470\n",
      "Iteration 95, loss = 0.02367461\n",
      "Iteration 96, loss = 0.02304029\n",
      "Iteration 97, loss = 0.02226000\n",
      "Iteration 98, loss = 0.02227641\n",
      "Iteration 99, loss = 0.02140478\n",
      "Iteration 100, loss = 0.02183620\n",
      "Iteration 101, loss = 0.02069521\n",
      "Iteration 102, loss = 0.02057822\n",
      "Iteration 103, loss = 0.01957821\n",
      "Iteration 104, loss = 0.01948523\n",
      "Iteration 105, loss = 0.01889411\n",
      "Iteration 106, loss = 0.01815744\n",
      "Iteration 107, loss = 0.01869619\n",
      "Iteration 108, loss = 0.01714221\n",
      "Iteration 109, loss = 0.01703403\n",
      "Iteration 110, loss = 0.01671028\n",
      "Iteration 111, loss = 0.01672862\n",
      "Iteration 112, loss = 0.01600617\n",
      "Iteration 113, loss = 0.01597807\n",
      "Iteration 114, loss = 0.01515626\n",
      "Iteration 115, loss = 0.01500192\n",
      "Iteration 116, loss = 0.01452129\n",
      "Iteration 117, loss = 0.01376503\n",
      "Iteration 118, loss = 0.01434162\n",
      "Iteration 119, loss = 0.01319613\n",
      "Iteration 120, loss = 0.01349699\n",
      "Iteration 121, loss = 0.01292707\n",
      "Iteration 122, loss = 0.01287013\n",
      "Iteration 123, loss = 0.01262598\n",
      "Iteration 124, loss = 0.01172234\n",
      "Iteration 125, loss = 0.01166377\n",
      "Iteration 126, loss = 0.01202302\n",
      "Iteration 127, loss = 0.01110193\n",
      "Iteration 128, loss = 0.01068223\n",
      "Iteration 129, loss = 0.01132921\n",
      "Iteration 130, loss = 0.01017741\n",
      "Iteration 131, loss = 0.01060352\n",
      "Iteration 132, loss = 0.01048857\n",
      "Iteration 133, loss = 0.01002230\n",
      "Iteration 134, loss = 0.00987322\n",
      "Iteration 135, loss = 0.00973208\n",
      "Iteration 136, loss = 0.00947991\n",
      "Iteration 137, loss = 0.00935001\n",
      "Iteration 138, loss = 0.00906624\n",
      "Iteration 139, loss = 0.00859095\n",
      "Iteration 140, loss = 0.00818412\n",
      "Iteration 141, loss = 0.00824811\n",
      "Iteration 142, loss = 0.00763023\n",
      "Iteration 143, loss = 0.00846711\n",
      "Iteration 144, loss = 0.00850372\n",
      "Iteration 145, loss = 0.00770393\n",
      "Iteration 146, loss = 0.00805466\n",
      "Iteration 147, loss = 0.00797383\n",
      "Iteration 148, loss = 0.00728267\n",
      "Iteration 149, loss = 0.00761092\n",
      "Iteration 150, loss = 0.00662186\n",
      "Iteration 151, loss = 0.00621049\n",
      "Iteration 152, loss = 0.00591664\n",
      "Iteration 153, loss = 0.00585689\n",
      "Iteration 154, loss = 0.00623860\n",
      "Iteration 155, loss = 0.00622805\n",
      "Iteration 156, loss = 0.00602522\n",
      "Iteration 157, loss = 0.00621122\n",
      "Iteration 158, loss = 0.00692366\n",
      "Iteration 159, loss = 0.00654681\n",
      "Iteration 160, loss = 0.00564212\n",
      "Iteration 161, loss = 0.00539761\n",
      "Iteration 162, loss = 0.00484970\n",
      "Iteration 163, loss = 0.00483831\n",
      "Iteration 164, loss = 0.00551683\n",
      "Iteration 165, loss = 0.00481377\n",
      "Iteration 166, loss = 0.00520551\n",
      "Iteration 167, loss = 0.00635738\n",
      "Iteration 168, loss = 0.00665571\n",
      "Iteration 169, loss = 0.00520358\n",
      "Iteration 170, loss = 0.00423180\n",
      "Iteration 171, loss = 0.00477280\n",
      "Iteration 172, loss = 0.00468805\n",
      "Iteration 173, loss = 0.00393070\n",
      "Iteration 174, loss = 0.00364674\n",
      "Iteration 175, loss = 0.00366385\n",
      "Iteration 176, loss = 0.00382191\n",
      "Iteration 177, loss = 0.00396383\n",
      "Iteration 178, loss = 0.00364315\n",
      "Iteration 179, loss = 0.00337261\n",
      "Iteration 180, loss = 0.00323545\n",
      "Iteration 181, loss = 0.00293708\n",
      "Iteration 182, loss = 0.00361923\n",
      "Iteration 183, loss = 0.00696218\n",
      "Iteration 184, loss = 0.00896549\n",
      "Iteration 185, loss = 0.01461685\n",
      "Iteration 186, loss = inf\n",
      "Iteration 187, loss = inf\n",
      "Iteration 188, loss = 0.01412408\n",
      "Iteration 189, loss = 0.00732662\n",
      "Iteration 190, loss = 0.00465969\n",
      "Iteration 191, loss = 0.00402979\n",
      "Iteration 192, loss = 0.00368998\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training accuracy: 1.0\n",
      "Validation accuracy: 0.8516545601291364\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Scale data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_Tx = scaler.transform(X_train)\n",
    "X_test_Tx = scaler.transform(X_test)\n",
    "\n",
    "# Fit models that require scaling (e.g. neural networks)\n",
    "hl_sizes = [150, 100, 80, 60, 40, 20]\n",
    "nn_clfs = [MLPClassifier(hidden_layer_sizes=(size,), random_state=2345, verbose=True) for size in hl_sizes]\n",
    "\n",
    "for num, nn_clf in enumerate(nn_clfs):\n",
    "    print(str(hl_sizes[num]) + '-unit network:')\n",
    "    nn_clf.fit(X_train_Tx, y_train.values.ravel())\n",
    "    print('Training accuracy: ' + str(nn_clf.score(X_train_Tx, y_train.values)))\n",
    "    print('Validation accuracy: ' + str(nn_clf.score(X_test_Tx, y_test.values)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
